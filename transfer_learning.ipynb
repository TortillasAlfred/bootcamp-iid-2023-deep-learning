{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning With Poutyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade poutyne # to install lastest poutyne version, if it's not already\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils import model_zoo\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from poutyne import set_seeds, Model, ModelCheckpoint, CSVLogger, Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Also, we need to set Pythons's, NumPy's and PyTorch's seeds by using Poutyne function so that our training is (almost) reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_dataset(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    tgz_filename = os.path.join(path, \"images.tgz\")\n",
    "\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://graal.ift.ulaval.ca/public/CUB200.tgz\", tgz_filename\n",
    "    )\n",
    "    print(\"Extracting archive...\")\n",
    "    archive = tarfile.open(tgz_filename)\n",
    "    archive.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./datasets/CUB200\"\n",
    "extract_dest = os.path.join(base_path, \"images\")\n",
    "download_and_extract_dataset(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_coefs = {}\n",
    "norm_coefs[\"cub200\"] = [\n",
    "    (0.47421962, 0.4914721, 0.42382449),\n",
    "    (0.22846779, 0.22387765, 0.26495799),\n",
    "]\n",
    "norm_coefs[\"imagenet\"] = [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*norm_coefs[\"cub200\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# is_valid_file removes hidden files from the dataset.\n",
    "dataset = ImageFolder(\n",
    "    extract_dest,\n",
    "    transform=transform,\n",
    "    is_valid_file=lambda path: not os.path.split(path)[1].startswith(\".\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We do the split train/valid/test with a 60/20/20 split respectively. We do a *stratified* split with scikit-learn in order to get examples of every class in every split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take 60% of the dataset for the training dataset\n",
    "train_indices, valid_test_indices = train_test_split(\n",
    "    np.arange(len(dataset)),\n",
    "    train_size=0.6,\n",
    "    stratify=dataset.targets,\n",
    "    random_state=42,\n",
    ")\n",
    "# We take 20% for the validation dataset and 20% for the test dataset\n",
    "# (i.e. 50% of the remaining 40%).\n",
    "valid_indices, test_indices = train_test_split(\n",
    "    valid_test_indices,\n",
    "    train_size=0.5,\n",
    "    stratify=np.asarray(dataset.targets)[valid_test_indices],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "valid_dataset = Subset(dataset, valid_indices)\n",
    "test_dataset = Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's set our training constants. We first have the CUDA device used for training if one is present. Secondly, we set the number of classes (i.e. one for each number). Finally, we set the batch size (i.e. the number of elements to see before updating the model), the learning rate for the optimizer, and the number of epochs (i.e. the number of times we see the full dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training constants\n",
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = 200\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "n_epoch = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the PyTorch's dataloader to split our data into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=8, shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a pretrained `ResNet-18` networks and replace the head with the number of neurons equal to our number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We freeze the network except for its head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_weights(resnet18):\n",
    "    for name, param in resnet18.named_parameters():\n",
    "        if not name.startswith(\"fc.\"):\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "freeze_weights(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We define callbacks for saving last epoch, best epoch and logging the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We are saving everything into ./saves/cub200.\n",
    "save_base_dir = \"saves\"\n",
    "save_path = os.path.join(save_base_dir, \"cub200\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    # Save the latest weights to be able to resume the optimization at the end for more epochs.\n",
    "    ModelCheckpoint(os.path.join(save_path, \"last_epoch.ckpt\")),\n",
    "    # Save the weights in a new file when the current model is better than all previous models.\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(save_path, \"best_epoch_{epoch}.ckpt\"),\n",
    "        monitor=\"val_acc\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        restore_best=True,\n",
    "        verbose=True,\n",
    "    ),\n",
    "    # Save the losses and accuracies for each epoch in a TSV.\n",
    "    CSVLogger(os.path.join(save_path, \"log.tsv\"), separator=\"\\t\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start the training and output its final test loss, accuracy, and micro F1-score.\n",
    "\n",
    "> Note that the F1-score is quite similar to the accuracy since the dataset is very balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(resnet18.fc.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "model = Model(\n",
    "    resnet18,\n",
    "    optimizer,\n",
    "    loss_function,\n",
    "    batch_metrics=[\"accuracy\"],\n",
    "    epoch_metrics=[\"f1\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model.fit_generator(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    epochs=n_epoch,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "test_loss, (test_acc, test_f1) = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logs = pd.read_csv(os.path.join(save_path, \"log.tsv\"), sep=\"\\t\")\n",
    "print(logs)\n",
    "\n",
    "best_epoch_idx = logs[\"val_acc\"].idxmax()\n",
    "best_epoch = int(logs.loc[best_epoch_idx][\"epoch\"])\n",
    "print(\"Best epoch: %d\" % best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = [\"loss\", \"val_loss\"]\n",
    "plt.plot(logs[\"epoch\"], logs[metrics])\n",
    "plt.legend(metrics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics = [\"acc\", \"val_acc\"]\n",
    "plt.plot(logs[\"epoch\"], logs[metrics])\n",
    "plt.legend(metrics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have created checkpoints using callbacks, we can restore the best model from those checkpoints and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=False, num_classes=num_classes)\n",
    "\n",
    "model = Model(\n",
    "    resnet18,\n",
    "    \"sgd\",\n",
    "    \"cross_entropy\",\n",
    "    batch_metrics=[\"accuracy\"],\n",
    "    epoch_metrics=[\"f1\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model.load_weights(\n",
    "    os.path.join(save_path, \"best_epoch_{epoch}.ckpt\").format(epoch=best_epoch)\n",
    ")\n",
    "\n",
    "test_loss, (test_acc, test_f1) = model.evaluate_generator(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the Experiment class to train our network. This class saves checkpoints and logs as above in a directory and allows to stop and resume optimization at will. See documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_train(epochs):\n",
    "    # Reload the pretrained network and freeze it except for its head.\n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "    resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
    "    freeze_weights(resnet18)\n",
    "\n",
    "    # Saves everything into ./saves/cub200_resnet18_experiment\n",
    "    save_path = os.path.join(save_base_dir, \"cub200_resnet18_experiment\")\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        resnet18.fc.parameters(), lr=learning_rate, weight_decay=0.001\n",
    "    )\n",
    "\n",
    "    # Poutyne Experiment\n",
    "    exp = Experiment(\n",
    "        save_path,\n",
    "        resnet18,\n",
    "        device=device,\n",
    "        optimizer=optimizer,\n",
    "        task=\"classif\",\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    exp.train(train_loader, valid_loader, epochs=epochs)\n",
    "\n",
    "    # Test\n",
    "    exp.test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment_train(epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train for 5 more epochs (10 epochs total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Retrain for 5 more epochs\n",
    "experiment_train(epochs=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
